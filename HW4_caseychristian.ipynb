{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the velocity field data\n",
    "\n",
    "Re = 600 # Reynolds number, change this to 300, 600, 900, 1200\n",
    "\n",
    "# Load the two-dimensional velocity field data. Data is stored in a 4D numpy array,\n",
    "# where the first dimension is the time index, the second and third dimensions are the\n",
    "# x and y coordinates, and the fourth dimension is the velocity components (ux or uv).\n",
    "vfield = np.load(\n",
    "    f\"../resources/von_karman_street/vortex_street_velocities_Re_{Re}_largefile.npz\", \n",
    "    allow_pickle=True\n",
    ")\n",
    "# downsample the data for faster training\n",
    "vfield = vfield[::8, ::4, ::4]\n",
    "print(\"Velocity field data has shape: {}\".format(vfield.shape))\n",
    "\n",
    "# Compute the velocity magnitude\n",
    "vfield_mag = np.sqrt(vfield[..., 0]**2 + vfield[..., 1]**2)\n",
    "\n",
    "n_tpts = vfield.shape[0]\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    v_vals = vfield_mag[n_tpts // 8 * i]\n",
    "    plt.imshow(v_vals, cmap=\"viridis\", vmin=0, vmax=np.percentile(v_vals, 99))\n",
    "    plt.title(\"Timepoint {}\".format(i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.015s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class ForecastingDataset:\n",
    "    \"\"\"\n",
    "    A class for formatting time series data for forecasting.\n",
    "    By convention, time is assumed to be the first dimension of the dataset.\n",
    "\n",
    "    For time series data, it is very important that all data in test set is after \n",
    "    all data in the train and val sets. We also need to ensure that datapoints don't\n",
    "    appear in multiple splits. \n",
    "\n",
    "    Parameters\n",
    "        X (np.ndarray): The time series data. The first dimension is assumed to be time.\n",
    "        split_ratio (tuple): The ratio of the data to be used for train, val, and test.\n",
    "        forecast_horizon (int): The number of time steps to forecast at once\n",
    "        featurizer (callable): A function that takes in multivariate snapshot and \n",
    "            returns a feature vector. If None, the raw data is used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, split_ratio=(0.6, 0.2, 0.2), forecast_horizon=1, featurizer=None):\n",
    "\n",
    "        if featurizer is None:\n",
    "            self.featurizer = lambda x: x\n",
    "        else:\n",
    "            self.featurizer = featurizer\n",
    "\n",
    "        self.feature_shape = X.shape[1:]\n",
    "\n",
    "        # We need to ensure that datapoints don't appear in multiple splits, hence why\n",
    "        # we crop by the forecast horizon. We are going to do one-step forecasting\n",
    "        self.X_full = self.featurizer(X[:-1])#[:-forecast_horizon]\n",
    "        self.y_full = X[1:]#[forecast_horizon:]\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        # Split the data into train, val, test\n",
    "        n_train = int(len(self.X_full) * split_ratio[0])\n",
    "        n_val = int(len(self.X_full) * split_ratio[1])\n",
    "        n_test = len(self.X_full) - n_train - n_val\n",
    "\n",
    "        # Our frequent use of the forecast_horizon parameter again arises from our need\n",
    "        # to ensure that datapoints don't appear in multiple splits.\n",
    "        self.X_train, self.y_train = self.X_full[:n_train], self.y_full[:n_train]\n",
    "        self.X_val, self.y_val = (\n",
    "            self.X_full[n_train + forecast_horizon : n_train + forecast_horizon + n_val], # 1 - 600, 2 - 601 | 601 - 800\n",
    "            self.y_full[n_train+ forecast_horizon:n_train + forecast_horizon + n_val]\n",
    "        )\n",
    "        self.X_test, self.y_test = (\n",
    "            self.X_full[n_train + 2 * forecast_horizon+ n_val:], \n",
    "            self.y_full[n_train + 2 * forecast_horizon + n_val:]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_full)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_full[idx], self.y_full[idx]\n",
    "\n",
    "    def flatten_data(self, x):\n",
    "        \"\"\"\n",
    "        Given a dataset, transform into a flat feature form\n",
    "        \"\"\"\n",
    "        return np.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "    def unflatten_data(self, x):\n",
    "        \"\"\"\n",
    "        Given a flat dataset, convert back to the original shape\n",
    "        \"\"\"\n",
    "        out = np.reshape(x, (x.shape[0], *self.feature_shape))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Let's do a simple unit test to make sure that our class is working as expected\n",
    "# Take some time to understand what these test cases cover\n",
    "import unittest\n",
    "class TestForecastingDataset(unittest.TestCase):\n",
    "\n",
    "    def test_initialization(self):\n",
    "        fd = ForecastingDataset(np.arange(100)[:, None])\n",
    "        assert fd.y_train[0] == fd.X_train[1], \"y_train is not shifted by 1 from X_train\"\n",
    "        assert fd.y_val[0] == fd.X_val[1], \"y_val is not shifted by 1 from X_val\"\n",
    "        assert fd.y_test[0] == fd.X_test[1], \"y_test is not shifted by 1 from X_test\"\n",
    "        \n",
    "        assert fd.y_train[-1] < fd.y_val[0], \"y_train and y_test are not disjoint\"\n",
    "        assert fd.y_val[-1] < fd.y_test[0], \"y_val and y_test are not disjoint\"\n",
    "\n",
    "    # test split_size\n",
    "\n",
    "unittest.main(argv=[''], exit=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "class BaseRegressor:\n",
    "    \"\"\"\n",
    "    A base class for regression models.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the model to the data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.weights + self.bias\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the mean squared error of the model.\n",
    "        \"\"\"\n",
    "        return np.mean((self.predict(X) - y)**2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LinearRegressor(BaseRegressor):\n",
    "    \"\"\"\n",
    "    A linear regression model is a linear function of the form:\n",
    "    y = w0 + w1 * x1 + w2 * x2 + ... + wn * xn\n",
    "    The weights are the coefficients of the linear function.\n",
    "    The bias is the constant term w0 of the linear function.\n",
    "    Attributes:\n",
    "        method: str, optional. The method to use for fitting the model.\n",
    "        regularization: str, optional. The type of regularization to use.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method=\"global\", regularization=\"ridge\", regstrength=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.method = method\n",
    "        self.regularization = regularization\n",
    "        self.regstrength = regstrength\n",
    "\n",
    "    # functions that begin with underscores are private, by convention.\n",
    "    # Technically we could access them from outside the class, but we should\n",
    "    # not do that because they can be changed or removed at any time.\n",
    "    def _fit_global(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the model using the global least squares method.\n",
    "        \"\"\"\n",
    "        if self.regularization is None:\n",
    "            self.weights = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        elif self.regularization == \"ridge\":\n",
    "            self.weights = np.linalg.inv(X.T @ X + np.eye(X.shape[1]) * self.regstrength) @ X.T @ y\n",
    "        else:\n",
    "            warnings.warn(\"Unknown regularization method, defaulting to None\")\n",
    "            self.weights = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        self.bias = np.mean(y - X @ self.weights)\n",
    "        return self.weights, self.bias\n",
    "\n",
    "    def _fit_iterative(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent.\n",
    "        \"\"\"\n",
    "        self.weights = np.zeros((X.shape[1], X.shape[1]))\n",
    "        self.bias = np.mean(y)\n",
    "        for i in range(X.shape[0]):\n",
    "            self.weights += learning_rate * (y[i] - X[i] @ self.weights - self.bias) * X[i] - self.regstrength * self.weights\n",
    "        self.weights /= X.shape[0]\n",
    "        return self.weights, self.bias\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the model to the data. The method used is determined by the\n",
    "        `method` attribute.\n",
    "        \"\"\"\n",
    "        if self.method == \"global\":\n",
    "            out = self._fit_global(X, y)\n",
    "        elif self.method == \"iterative\":\n",
    "            out = self._fit_iterative(X, y)\n",
    "        else:\n",
    "            out = self._fit_global(X, y)\n",
    "        return out\n",
    "\n",
    "def featurize_flowfield(field):\n",
    "    \"\"\"\n",
    "    Compute features of a 2D spatial field. These features are chosen based on the \n",
    "    intuition that the input field is a 2D spatial field with time translation \n",
    "    invariance.\n",
    "    The output is an augmented feature along the last axis of the input field.\n",
    "    Args:\n",
    "        field (np.ndarray): A 3D array of shape (batch, nx, ny) containing the flow field\n",
    "    Returns:\n",
    "        field_features (np.ndarray): A 3D array of shape (batch, nx, ny, M) containing \n",
    "            the computed features stacked along the last axis\n",
    "    \"\"\"\n",
    "    field_fft = np.fft.fft2(field)\n",
    "    field_fft = np.fft.fftshift(field_fft)\n",
    "    field_fft_abs = np.log(np.abs(field_fft) + 1e-8)[..., None]\n",
    "    field_fft_phase = np.angle(field_fft)[..., None]\n",
    "\n",
    "    field_grad = np.gradient(field, axis=(-2, -1))\n",
    "    field_grad = np.stack(field_grad, axis=-1)\n",
    "\n",
    "    field_lap = np.stack(np.gradient(field_grad, axis=(-2, -1)), axis=-1)\n",
    "    field_lap = np.sum(field_lap, axis=-1)\n",
    "\n",
    "    field = field[..., None]\n",
    "    field_features = np.concatenate(\n",
    "        [field, field_grad, field_lap, field_fft_phase, field_fft_abs], \n",
    "        axis=-1\n",
    "    )\n",
    "    return field_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regstrengths = np.logspace(-4, 4, 9)\n",
    "\n",
    "all_mse = []\n",
    "for regstrength in regstrengths:\n",
    "    print(f\"Training model with regstrength={regstrength}\", flush=True)\n",
    "\n",
    "    model = LinearRegressor(method=\"global\", regularization=\"ridge\", regstrength=regstrength)\n",
    "\n",
    "    model.fit(\n",
    "        dataset.flatten_data(dataset.X_train), \n",
    "        dataset.flatten_data(dataset.y_train)\n",
    "    )\n",
    "\n",
    "    y_val_pred = dataset.unflatten_data(\n",
    "        model.predict(dataset.flatten_data(dataset.X_val))\n",
    "    )\n",
    "\n",
    "    mse = np.mean((y_val_pred - dataset.y_val)**2)\n",
    "    all_mse.append(mse)\n",
    "\n",
    "best_regstrength = regstrengths[np.argmin(np.array(all_mse))]\n",
    "\n",
    "print(f\"Best regularize strength: {best_regstrength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressor(method=\"global\", regularization=\"ridge\", regstrength=best_regstrength)\n",
    "\n",
    "model.fit(\n",
    "    dataset.flatten_data(dataset.X_train), \n",
    "    dataset.flatten_data(dataset.y_train)\n",
    ")\n",
    "\n",
    "y_test_pred = dataset.unflatten_data(\n",
    "    model.predict(dataset.flatten_data(dataset.X_test))\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(np.hstack(y_test_pred[::3][-10:]))\n",
    "plt.title(\"Predicted snapshots\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(np.hstack(dataset.y_test[::3][-10:]))\n",
    "plt.title(\"True snapshots\")\n",
    "\n",
    "mse_vs_time = np.mean((y_test_pred - dataset.y_test)**2, axis=(1, 2))\n",
    "plt.figure()\n",
    "plt.plot(mse_vs_time)\n",
    "plt.ylabel(\"One step ahead forecast error\")\n",
    "plt.xlabel(\"Time step\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
